install.packages("tidyverse")
prices
prices
prices <- c(Portugal=10.3, Spain=10.6, Italy=11.5, France=12.3, Germany=9.9, Greece=9.3, UK=11.4, Finland=10.9, Belgium=12.1, Austria=9.1)
prices
prices_with_vat <- 1.23*prices
prices_with_vat
prices[prices > 10]
prices[prices > mean(prices)]
prices[prices > 10 & prices < 11]
raised_prices <- 1.1 * prices
raised_prices
lower_prices <- prices[prices > mean(prices)] * 0,975
lower_prices <- prices[prices > mean(prices)] * 0.975
lower_prices
euro2currency <- function(value, currency) value * exchange_rate["currency"]
euro2currency(3, usd)
euro2currency(3, "usd")
exchange_rate <- c(usd=1.0458, gbp=0.8276, jpy=156.11, chf=0.9389, huf=403.0416)
euro2currency <- function(value, currency) value * exchange_rate["currency"]
euro2currency(2, "usd")
euro2currency(2, usd)
euro2currency <- function(value, currency) value * exchange_rate[currency]
euro2currency(2, usd)
euro2currency(2, "usd")
amounts <- [50, 100, 150]
euro2currency(amounts, "chf")
amounts <- [50, 100, 150]
amounts <- c(50, 100, 150)
euro2currency(amounts, "chf")
help(Boston, package='MASS')
data(Boston, package='MASS')
prices <- data(Boston, package='MASS')
prices
packages.install('MASS')
install.packages('MASS')
prices <- data(Boston, package='MASS')
prices
library(MASS)
data(Boston)
str(Boston)
data(Boston, package = 'MASS')
str(Boston)
prices <- data(Boston, package = 'MASS')
str(prices)
str(Boston)
data(Boston, package = 'MASS')
subset(Boston, medv > 45)
subset(Boston, rm > 8, c(nox, tax))
subset(Boston, medv > 10 & medv < 15)
subset(Boston, medv >= 10 & medv <= 15)
subset(Boston, rm > 6, c(mean(crim)))
mean(Boston$crim(Boston$rm > 6))
mean(Boston$crim[Boston$rm > 6])
install.packages("dplyr")
library(arules)
library(arulesViz)
install.packages('arulesViz')
library(arulesViz)
data(Groceries)
Groceries
class(Groceries)
# Get information on the dataset
summary(Groceries)
# Get the size of the dataset
size(Groceries)
# Get the size of the dataset
head(size(Groceries))
# Inspect the dataset and its type
Groceries
class(Groceries)
# Get information on the dataset
summary(Groceries)
# Get the size of the dataset
head(size(Groceries))
# Get the first five transactions
inspect(Groceries[1:5])
size(Groceries)
# Check if there are duplicated transactions
unique(Groceries)
# Check if there are duplicated transactions
length(which(unique(Groceries)))
# Check if there are duplicated transactions
length(which(duplicated(Groceries)))
# See the relative frequency of each item
head(itemFrequency(Groceries))
# Plot the top 5 more frequent items
head(itemFrequencyPlot(Groceries))
# Plot the top 5 more frequent items
itemFrequencyPlot(Groceries, topN = 5)
# Plot the items that have a support value of at least 0.1
itemFrequencyPlot(Groceries, support = 0.1)
# Obtain the frequent itemsets for a minimum support of 0.01
itemsets <- apriori(Groceries, parameter = list(supp = 0.1, target = "frequent itemsets"))
class(itemsets)
class(itemsets)
class(Groceries)
# Inspect the 5 most frequent itemsets
inspect(itemsets[1:5])
# Inspect the 5 most frequent itemsets
inspect(sort(itemsets)[1:5])
# Select the subset of closed frequent itemsets and the subset of maximal frequent itemsets from the frequent itemsets obtained
itemsets[is.closed(itemsets)]
itemsets[is.maximal(itemsets)]
# Generate the rules from the dataset using apriori
rules <- apriori(Groceries)
class(rules)
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3, minlen = 2, target = "rules"))
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3)
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3))
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3))
# Generate the rules from the dataset using apriori and check its class
rules <- apriori(Groceries)
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3))
# Obtain the rules with minsup = 0.01 and minconf = 0.25
rules <- apriori(Groceries, parameter = list(supp = 0.01, conf = 0.25))
summary(rules)
quality(rules)
quality(sort(rules))
quality(rules[1:5])
plot(rules[1:5])
inspect(rules[1:5])
# Select the rules with a lift value above 2
rules.sub <- subset(rules, subset = lift > 2)
inspect(rules.sub[1:5])
# Select the rules that have lift value above 2 and the items "whole milk" or "yogurt" on the consequent
rules.sub <- subset(rules, subset = rhs %in% c("yogurt", "whole milk") & lift > 2)
rules.sort <- sort(rules.sub, by "lift")
rules.sort <- sort(rules.sub, by = "lift")
inspect(rules.sort[1:5])
plot(rules.sub)
plot(rules.sub, method = "graph")
# Set the working directory dinamically
wd <- readline(prompt = "Enter the path of your working directory: ")
setwd(wd)
# Load required packages
library(recommenderlab)
library(ggplot2)
# Read the csv file into a data frame
df <- read.csv("log1.csv")
# Convert the data frame to a binary matrix
brm <- as(df, "binaryRatingMatrix")
# Set the seed to 2021
set.seed(2021)
# Check the number of ratings per user
user_item_counts <- rowCounts(brm)
# Identify users with at least 3 ratings
valid_users <- names(user_item_counts[user_item_counts >= 3])
# Create a filtered matrix with the valid users
filtered_brm <- brm[valid_users]
# Define an evaluation scheme that splits the data into train and test set and establish that 2 items of test cases are already known
eval_scheme <- evaluationScheme(data = filtered_brm, method = "split", train = 0.8, given = 2, goodRating = 1)
# Check how the data was split according to the previous evaluation scheme
train_data <- getData(eval_scheme, "train")
print(dim(train_data))
known_data <- getData(eval_scheme, "known")
print(dim(known_data))
unknown_data <- getData(eval_scheme, "unknown")
print(dim(unknown_data))
# Define the list of methods that will be used to obtain the top N recommendations
methods <- list("popular" = list(name="POPULAR", param=NULL), "user-based CF" = list(name="UBCF", param=NULL), "item-based CF" = list(name="IBCF", param=NULL))
# Use the function evaluate with the previously defined evaluation scheme, methods and considering top 1, 3 and 5 recommendations for each of the models
results <- evaluate(eval_scheme, methods, n = c(1,3,5))
# Explore the obtained object
print(str(results))
print(avg(results))
# Obtain the corresponding confusion matrices
for(method in names(methods)){
for(n_val in c(1,3,5)){
eval_result <- results[[method]] [[as.character(n_val)]]
}
}
eval_result <- results[[method]]
eval_result <- results[[method]]
for(method in names(methods)){
print(results[[method]])
}
# Obtain the corresponding confusion matrices
results_df <- getConfusionMatrix(results)
# Obtain the corresponding confusion matrices
for(algo_name in names(results@results)){
algo_results <- results@results[[algo_name]]
algo_results <- results@results[[algo_name]]
for(n_idx in 1:length(algo_results)){
n_value <- names(algo_results)[n_idx]
eval_result <- algo_results[[n_idx]]
tryCatch({
conf_matrix <- getConfusionMatrix(eval_result)
print(conf_matrix)
tp <- conf_matrix["TP"]
fp <- conf_matrix["FP"]
fn <- conf_matrix["FN"]
tn <- conf_matrix["TN"]
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
cat("Precision:", round(precision, 4), "Recall:", round(recall, 4), "\n")
}, error = function(e) {
cat("Error getting confusion matrix", e$message, "\n")
tryCatch({
cm <- eval_result@cm
tp <- cm["CM"]
fp <- cm["FP"]
fn <- cm["FN"]
tn <- cm["TN"]
precision <- tp / (tp + fp)
recall <- tp / (tp + fn)
cat("Precision:", round(precision, 4), "Recall:", round(recall, 4), "\n")
}, error = function(e2) {
cat("Failed access to CM:", e2$message, "\n")
})
})
}
}
# Plot the ROC curves
plot(results, "ROC", annotate = TRUE, main = "ROC Curves")
# Plot the precision/recall curves for each of the methods
plot(results, "prec/rec", annotate = TRUE, main = "Precision-Recall Curve")
# Load required packages
library(quanteda)
install.packages(quanteda)
install.packages("quanteda")
# Load required packages
library(quanteda)
library(quanteda.textplots)
install.packages("quanteda.textplots")
# Load required packages
library(quanteda)
library(quanteda.textplots)
# Create a corpus with the documents
docs <- c(
doc1 = "Mining is important for finding gold",
doc2 = "Classification and regression are data mining",
doc3 = "Data mining deals with data"
)
corp <- corpus(docs)
# Tokenize and create a document-feature matrix
toks <- tokens(corp)
dfm <- dfm(corp)
dfm <- dfm(toks(corp))
# Tokenize and create a document-feature matrix
toks <- tokens(corp)
dfm <- dfm(toks(corp))
dfm <- dfm(toks)
# Get the information on the dfm
ndoc(dfm)
nfeat(dfm)
featnames(dfm)
# Inspect the dfm
dfm
as.matrix(dfm)
# Create a version of the dfm with a binary weighting
dfm_binary <- dfm_weight(dfm, scheme = "boolean")
as.matrix(dfm_binary)
# Create a version of the dfm with TF-IDF weighting
dfm_tfidf <- dfm_tfidf(dfm)
as.matrix(dfm_tfidf)
# Check terms with zero value for all documents
all_zeros_binary <- which(colSums(as.matrix(dfm_binary)) == 0)
all_zeros_tfidf <- which(colSums(as.matrix(dfm_tfidf)) == 0)
all_zeros_binary
print(all_zeros_binary)
all_zeros_tfidf
# Analyze the cosine similarity between the three documents for each weighting scheme
sim_binary <- textstat_simil(dfm_binary, method = "cosine")
# Load required packages
library(quanteda)
library(quanteda.textplots)
# Analyze the cosine similarity between the three documents for each weighting scheme
sim_binary <- textstat_simil(dfm_binary, method = "cosine")
install.packages("quanteda.textstats")
library(quanteda.textstats)
# Analyze the cosine similarity between the three documents for each weighting scheme
sim_binary <- textstat_simil(dfm_binary, method = "cosine")
as.matrix(sim_binary)
sim_tfidf <- textstat_simil(dfm_tfidf, method = "cosine")
as.matrix(sim_tfidf)
# Rank the documents given the query "data mining" by the cosine similarity of the query to each document using the binary-weighted dfm
query <- "data mining"
query_corpus <- corpus(query)
query_tokens <- tokens(query_corpus)
query_dfm <- dfm(query_tokens)
query_dfm_matched <- dfm_match(query_dfm, featnames(dfm))
query_binary <- dfm_weight(query_dfm_matched, scheme = "boolean")
sim_query_binary <- textstat_simil(dfm_binary, query_binary, method = "cosine")
sim_query_binary
# Rank the documents given the query "data mining" by the cosine similarity of the query to each document using TF weighting
sim_query_tf <- textstat_simil(dfm, query_dfm_matched, method = "cosine")
sim_query_tf
# Rank the documents given the query "data mining" by the cosine similarity of the query to each document using TF-IDF weighting
query_tfidf <- dfm_tfidf(query_dfm_matched)
sim_query_tfidf <- textstat_simil(dfm_tfidf, query_tfidf, method = "cosine")
sim_query_tfidf
# Load the 'data_corpus_inaugural' dataset
inaug <- data_corpus_inaugural
# Inspect the first 5 documents of the loaded corpus
head(inaug)
# Inspect the first 5 documents of the loaded corpus
head(summary(inaug), 5)
# Inspect the first 5 documents of the loaded corpus
head(inaug, 5)
# Inspect the first 5 documents of the loaded corpus
head(summary(inaug), 5)
# Obtain a graphical representation of the terms in the corpus
inaug_tokens <- tokens(inaug)
