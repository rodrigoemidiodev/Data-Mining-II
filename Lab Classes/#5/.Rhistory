install.packages("tidyverse")
prices
prices
prices <- c(Portugal=10.3, Spain=10.6, Italy=11.5, France=12.3, Germany=9.9, Greece=9.3, UK=11.4, Finland=10.9, Belgium=12.1, Austria=9.1)
prices
prices_with_vat <- 1.23*prices
prices_with_vat
prices[prices > 10]
prices[prices > mean(prices)]
prices[prices > 10 & prices < 11]
raised_prices <- 1.1 * prices
raised_prices
lower_prices <- prices[prices > mean(prices)] * 0,975
lower_prices <- prices[prices > mean(prices)] * 0.975
lower_prices
euro2currency <- function(value, currency) value * exchange_rate["currency"]
euro2currency(3, usd)
euro2currency(3, "usd")
exchange_rate <- c(usd=1.0458, gbp=0.8276, jpy=156.11, chf=0.9389, huf=403.0416)
euro2currency <- function(value, currency) value * exchange_rate["currency"]
euro2currency(2, "usd")
euro2currency(2, usd)
euro2currency <- function(value, currency) value * exchange_rate[currency]
euro2currency(2, usd)
euro2currency(2, "usd")
amounts <- [50, 100, 150]
euro2currency(amounts, "chf")
amounts <- [50, 100, 150]
amounts <- c(50, 100, 150)
euro2currency(amounts, "chf")
help(Boston, package='MASS')
data(Boston, package='MASS')
prices <- data(Boston, package='MASS')
prices
packages.install('MASS')
install.packages('MASS')
prices <- data(Boston, package='MASS')
prices
library(MASS)
data(Boston)
str(Boston)
data(Boston, package = 'MASS')
str(Boston)
prices <- data(Boston, package = 'MASS')
str(prices)
str(Boston)
data(Boston, package = 'MASS')
subset(Boston, medv > 45)
subset(Boston, rm > 8, c(nox, tax))
subset(Boston, medv > 10 & medv < 15)
subset(Boston, medv >= 10 & medv <= 15)
subset(Boston, rm > 6, c(mean(crim)))
mean(Boston$crim(Boston$rm > 6))
mean(Boston$crim[Boston$rm > 6])
install.packages("dplyr")
library(arules)
library(arulesViz)
install.packages('arulesViz')
library(arulesViz)
data(Groceries)
Groceries
class(Groceries)
# Get information on the dataset
summary(Groceries)
# Get the size of the dataset
size(Groceries)
# Get the size of the dataset
head(size(Groceries))
# Inspect the dataset and its type
Groceries
class(Groceries)
# Get information on the dataset
summary(Groceries)
# Get the size of the dataset
head(size(Groceries))
# Get the first five transactions
inspect(Groceries[1:5])
size(Groceries)
# Check if there are duplicated transactions
unique(Groceries)
# Check if there are duplicated transactions
length(which(unique(Groceries)))
# Check if there are duplicated transactions
length(which(duplicated(Groceries)))
# See the relative frequency of each item
head(itemFrequency(Groceries))
# Plot the top 5 more frequent items
head(itemFrequencyPlot(Groceries))
# Plot the top 5 more frequent items
itemFrequencyPlot(Groceries, topN = 5)
# Plot the items that have a support value of at least 0.1
itemFrequencyPlot(Groceries, support = 0.1)
# Obtain the frequent itemsets for a minimum support of 0.01
itemsets <- apriori(Groceries, parameter = list(supp = 0.1, target = "frequent itemsets"))
class(itemsets)
class(itemsets)
class(Groceries)
# Inspect the 5 most frequent itemsets
inspect(itemsets[1:5])
# Inspect the 5 most frequent itemsets
inspect(sort(itemsets)[1:5])
# Select the subset of closed frequent itemsets and the subset of maximal frequent itemsets from the frequent itemsets obtained
itemsets[is.closed(itemsets)]
itemsets[is.maximal(itemsets)]
# Generate the rules from the dataset using apriori
rules <- apriori(Groceries)
class(rules)
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3, minlen = 2, target = "rules"))
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3)
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3))
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3))
# Generate the rules from the dataset using apriori and check its class
rules <- apriori(Groceries)
# Generate new rules with updated minimum support and confidence
rules_new <- apriori(Groceries, parameter = list(supp = 0.1, conf = 0.3))
# Obtain the rules with minsup = 0.01 and minconf = 0.25
rules <- apriori(Groceries, parameter = list(supp = 0.01, conf = 0.25))
summary(rules)
quality(rules)
quality(sort(rules))
quality(rules[1:5])
plot(rules[1:5])
inspect(rules[1:5])
# Select the rules with a lift value above 2
rules.sub <- subset(rules, subset = lift > 2)
inspect(rules.sub[1:5])
# Select the rules that have lift value above 2 and the items "whole milk" or "yogurt" on the consequent
rules.sub <- subset(rules, subset = rhs %in% c("yogurt", "whole milk") & lift > 2)
rules.sort <- sort(rules.sub, by "lift")
rules.sort <- sort(rules.sub, by = "lift")
inspect(rules.sort[1:5])
plot(rules.sub)
plot(rules.sub, method = "graph")
# Inspect how many times each page was visited
page_visits <- table(df$PAGE)
# Set the working directory dinamically
wd <- readline(prompt = "Enter the path of your working directory: ")
setwd(wd)
# Read the csv file into a data frame
df <- read.csv("log.csv")
# Inspect how many times each page was visited
page_visits <- table(df$PAGE)
print(page_visits)
# Sort the pages by decreasing number of visits
sorted_pages <- sort(page_visits, decreasing = TRUE)
print(sorted_pages)
# Obtain the top 3 pages for recommendation
top_3_pages <- names(sorted_pages)[1:3]
print(top_3_pages)
# Set the working directory dinamically
wd <- readline(prompt = "Enter the path of your working directory: ")
setwd(wd)
# Read the csv file into a data frame
df <- rxead.csv("log.csv")
# Read the csv file into a data frame
df <- read.csv("log.csv")
# Transform the log access data into a matrix with a row for each user and for each user the information on his visits to this page
user_page_matrix <- table(log_data$USER, log_data$PAGE)
# Transform the log access data into a matrix with a row for each user and for each user the information on his visits to this page
user_page_matrix <- table(df$USER, df$PAGE)
print(user_page_matrix)
# Obtain a distance matrix with the Euclidean distance between the users
dist_matrix <- dist(user_page_matrix, method = "euclidean")
print(dist_matrix)
# Visualize the obtained dendogram
plot(hclust_model)
# Obtain an agglomerative clustering model with the distance matrix
hclust_model <- hclust(dist_matrix, method = "complete")
# Visualize the obtained dendogram
plot(hclust_model)
# Visualize the obtained dendogram
plot(hclust_model, main = "Hierarchical Clustering Dendogram")
# Visualize the dendogram with hang = -0.1
plot(hclust_model, hang = -0.1, main = "Hierarchical Clustering Dendogram with hang = -0.1")
# Visualize the obtained dendogram
plot(hclust_model, main = "Hierarchical Clustering Dendogram")
# Visualize the dendogram with hang = -0.1
plot(hclust_model, hang = -0.1, main = "Hierarchical Clustering Dendogram with hang = -0.1")
# Cut the hierarchical clustering in two clusters and inspect the cluster membership of each user
cluster <- cutree(hclust_model, k = 2)
# Cut the hierarchical clustering in two clusters and inspect the cluster membership of each user
clusters <- cutree(hclust_model, k = 2)
print(clusters)
# Draw the previous solution in the dendogram
rect.hclust(hclust_model, k = 2, border = "red")
# Inspect the pages visited by users in cluster 1
cluster1_users <- names(clusters[clusters == 1])
print(cluster1_users)
# Inspect how many times each of the pages was visited
cluster1_data <- df[df$USER %in% cluster1_users, ]
print(table(cluster1_data$USER, cluster1_data$PAGE))
# Inspect how many times each page was visited
cluster1_page_visits <- table(cluster1_data$PAGE)
print(cluster1_page_visits)
# Sort the pages by decreasing order of visits
sorted_cluster1_pages <- sort(cluster1_page_visits, decreasing = TRUE)
print(sorted_cluster1_pages)
# Obtain the top 2 pages for recommendation
top2_pages_cluster1 <- names(sorted_cluster1_pages)[1:2]
print(top2_pages_cluster1)
# Recommend the top 2 pages for users of cluster 2
cluster2_users <- names(clusters[clusters == 2])
print(cluster2_users)
cluster2_data <- df[df$USER %in% cluster2_users, ]
print(table(cluster2_data$USER, cluster2_data$PAGE))
cluster2_page_visits <- table(cluster2_data$PAGE)
print(cluster2_page_visits)
sorted_cluster2_pages <- sort(cluster2_page_visits, decreasing = TRUE)
print(sorted_cluster2_pages)
top2_pages_cluster2 <- names(sorted_cluster2_pages)[1:2]
print(top2_pages_cluster2)
# Recommend the top 3 pages for user u2, removing the pages that the user has already visited
u2_cluster <- clusters["u2"]
# Recommend the top 3 pages for user u2, removing the pages that the user has already visited
u2_cluster <- clusters["u2"]
print(u2_cluster)
u2_visited_pages <- df$PAGE[df$USER == "u2"]
print(u2_visited_pages)
recommendation_candidates <- names(cluster_pages)[!names(cluster_pages) %in% u2_visited_pages]
recommendation_candidates <- names(sorted_cluster1_pages)[!names(sorted_cluster1_pages) %in% u2_visited_pages]
top_n <- min(3, length(recommendation_candidates))
top_pages_for_u2 <- recommendation_candidates[1:top_n]
print(top_pages_for_u2)
print(recommendation_candidates)
# Set the working directory dinamically
wd <- readline(prompt = "Enter the path of your working directory: ")
setwd(wd)
# Read the csv file into a data frame
df <- read.csv("log1.csv")
# Load required packages
library(recommenderlab)
install.packages("recommenderlab")
# Load required packages
library(recommenderlab)
# Read the csv file into a data frame
df <- read.csv("log1.csv")
# Coerce the data frame with user-page access information from log1.csv file to a binaryRatingMatrix
brm <- as(df, "binaryRatingMatrix")
# Select the information on the first 6 users to be used as training offline data and save it to a new variable
brm_offline <- brm[1:6]
# Inspect the content of brm_offline
brm_matrix <- getRatingMatrix(brm_offline)
brm_df <- as(brm_offline, "data.frame")
head(brm_df)
# Apply the function rowCounts and colCounts to brm_offline
rowCounts(brm_offline)
# Apply the function rowCounts and colCounts to brm_offline
user_counts <- rowCounts(brm_offline)
print(user_counts)
page_counts <- colCounts(brm_offline)
print(page_counts)
# Apply the function image to brm_offline
image(brm_offline, main = "User-Page Access Matrix")
# Obtain the recommender model based on association rules with the instruction modelAR <- Recommender(brm_offline, "AR")
AR_model <- getModel(modelAR)
# Obtain the recommender model based on association rules with the instruction modelAR <- Recommender(brm_offline, "AR")
modelAR <- Recommender(brm_offline, "AR")
# Inspect te association rules that compose the model
AR_model <- getModel(modelAR)
print(AR_model$rules)
print(AR_model)
print(AR_model$rule_base)
print(AR_model$rule_base)
print(AR_model$rules)
print(str(AR_model))
print(names(AR_model))
print(str(AR_model))
print(str(AR_model))
# Apply predict function and get the top 2 recommendations
u7 <- brm["u7"]
pred_u7 <- predict(modelAR, u7, n = 2)
print(pred_u7)
# Apply the function getList to the obtained predictions to inspect the actual recommendations
recom_u7 <- getList(pred_u7)
print(recom_u7)
# Filter the rules which have been triggered for this active user
u7_pages <- names(which(as(u7, "list")[[1]]))
# Filter the rules which have been triggered for this active user
u7_pages <- names(which(as(u7, "list")[1]))
]
# Filter the rules which have been triggered for this active user
u7_pages <- names(which(as(u7, "list")[[1]]))
# Filter the rules which have been triggered for this active user
u7_pages <- as(u7, "data.frame")
# Filter the rules which have been triggered for this active user
u7_df <- as(u7, "data.frame")
u7_pages <- as.character(u7_df$item[u7_df$rating == 1])
print(u7_pages)
if("itemsets" %in% names(AR_model)) {
print(AR_model$itemsets)
}
print(AR_model$confidence)
# Deploy the recommendation model for u8
u8 <- brm["u8"]
pred_u8 <- predict(modelAR, u8, n = 2)
recom_u8 <- getList(pred_u8)
print(recom_u8)
u8_df <- as(u8, "data.frame")
u8_pages <- as.character(u8_df$item[u8_df$rating == 1])
print(u8_pages)
# Explore the types of recommendation models available for binary rating matrices
models <- recommenderRegistry$get_entries(dataType="binaryRatingMatrix")
print(names(models))
# Make the top 2 recommendations to u7 and u8 using the popularity of the pages
modelPOP <- Recommender(brm_offline, "POPULAR")
pred_u7_pop <- predict(modelPOP, u7, n = 2)
recom_u7_pop <- getList(pred_u7_pop)
print(recom_u7_pop)
pred_u8_pop <- predict(modelPOP, u8, n = 2)
recom_n8_pop <- getList(pred_u8_pop)
recom_u8_pop <- getList(pred_u8_pop)
print(recom_u8_pop)
